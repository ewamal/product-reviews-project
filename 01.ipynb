{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from collections import Counter\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import datasets\n",
    "import numpy as np\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "\n",
    "datasets.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'nlptown/bert-base-multilingual-uncased-sentiment')\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'nlptown/bert-base-multilingual-uncased-sentiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    '/notebooks/Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products_May19.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data cleaning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove rows where 'reviews.text' is NaN or contains only whitespace\n",
    "\n",
    "df = df.dropna(subset=['reviews.text']).reset_index(drop=True)\n",
    "df['reviews.text'] = df['reviews.text'].astype(str)\n",
    "df = df[df['reviews.text'].str.strip().astype(bool)].reset_index(drop=True)\n",
    "\n",
    "columns_to_drop_df = ['dateAdded', 'dateUpdated', 'asins', 'reviews.date', 'manufacturerNumber']\n",
    "df.drop(columns=columns_to_drop_df, inplace=True, errors='ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = df['reviews.text'].tolist()\n",
    "df_reviews = pd.DataFrame(reviews, columns=['review'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "def sentiment_score(review):\n",
    "    with torch.no_grad():\n",
    "        tokens = tokenizer.encode(\n",
    "            review,\n",
    "            return_tensors='pt',\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        ).to(device)\n",
    "\n",
    "        result = model(tokens)\n",
    "\n",
    "        # Get the predicted class (0-4) and convert to 1-5 scale\n",
    "        score = int(torch.argmax(result.logits)) + 1\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "def map_score_to_classification(score):\n",
    "    if score <= 2:\n",
    "        return 'Negative'\n",
    "    elif score == 3:\n",
    "        return 'Neutral'\n",
    "    else:\n",
    "        return 'Positive'\n",
    "\n",
    "\n",
    "sentiment_scores = []\n",
    "sentiment_classifications = []\n",
    "\n",
    "for review in reviews:\n",
    "    score = sentiment_score(review)\n",
    "    classification = map_score_to_classification(score)\n",
    "\n",
    "    sentiment_scores.append(score)\n",
    "    sentiment_classifications.append(classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['categories_list'] = df['categories'].apply(\n",
    "    lambda x: [cat.strip() for cat in x.split(',') if cat.strip()])\n",
    "\n",
    "# Consolidate similar categories to eliminate redundancy\n",
    "consolidation_mapping = {\n",
    "    'All Bluetooth & Wireless Speakers': 'Bluetooth & Wireless Speakers',\n",
    "    'All Streaming Media Players': 'Streaming Media Players',\n",
    "    'All Tablets': 'Tablets',\n",
    "    'Amazon Tablets': 'Tablets',\n",
    "    'Amazon Book Reader': 'Book Readers',\n",
    "    'Amazon Book Reader Accessory': 'Book Reader Accessories',\n",
    "    'Amazon Device Accessories': 'Device Accessories',\n",
    "    'Amazon Devices': 'Devices',\n",
    "    'Amazon Devices & Accessories': 'Devices & Accessories',\n",
    "    'Amazon Ereaders': 'Ereaders',\n",
    "    'Amazon Tablet Accessory': 'Tablet Accessories',\n",
    "    'Amazon Tap': 'Amazon Tap',\n",
    "    'Audio Player Accessories': 'Audio Accessories',\n",
    "    'Carrying Case Or Bag': 'Carrying Cases & Bags',\n",
    "    'Cases & Bags': 'Cases & Bags',\n",
    "    'Cases & Covers': 'Cases & Covers',\n",
    "    'Carriers & Crates': 'Carriers & Crates',\n",
    "    'Carriers & Totes': 'Carriers & Totes',\n",
    "    'Cookware': 'Home & Kitchen',\n",
    "    'Kitchen & Dining': 'Home & Kitchen',\n",
    "    'Portable Audio & Headphones': 'Audio',\n",
    "    'Tablets': 'Computers & Tablets',\n",
    "    'iPads': 'Computers & Tablets',\n",
    "    'E-readers': 'Computers & Tablets',\n",
    "}\n",
    "\n",
    "df['categories_list'] = df['categories_list'].apply(\n",
    "    lambda cats: [consolidation_mapping.get(cat, cat) for cat in cats]\n",
    ")\n",
    "\n",
    "# Remove or assign to a meaningful category\n",
    "\n",
    "\n",
    "def clean_category(cat):\n",
    "    if cat in ['14701001', 'AA', 'AAA', 'Abis Electronics', 'Amazon', 'Amazon SMP']:\n",
    "        return None\n",
    "    return cat\n",
    "\n",
    "\n",
    "df['categories_list'] = df['categories_list'].apply(\n",
    "    lambda cats: [clean_category(cat)\n",
    "                  for cat in cats if clean_category(cat) is not None]\n",
    ")\n",
    "\n",
    "# Extract unique categories\n",
    "unique_categories = sorted(\n",
    "    set(cat for cats in df['categories_list'] for cat in cats))\n",
    "\n",
    "\n",
    "# Generate category embeddings using sentence transformers\n",
    "embedding_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "category_embeddings = embedding_model.encode(\n",
    "    unique_categories, show_progress_bar=True)\n",
    "category_embeddings = np.array(category_embeddings)\n",
    "\n",
    "# Cluster categories\n",
    "num_clusters = 6\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(category_embeddings)\n",
    "\n",
    "# Create category to cluster mapping\n",
    "category_cluster_df = pd.DataFrame({\n",
    "    'category': unique_categories,\n",
    "    'category_cluster': cluster_labels\n",
    "})\n",
    "\n",
    "# Map categories to clusters\n",
    "category_to_cluster = dict(\n",
    "    zip(category_cluster_df['category'], category_cluster_df['category_cluster']))\n",
    "\n",
    "# Assign primary cluster to each review\n",
    "\n",
    "\n",
    "def assign_primary_cluster(categories):\n",
    "    clusters = [category_to_cluster.get(\n",
    "        cat) for cat in categories if category_to_cluster.get(cat) is not None]\n",
    "    if not clusters:\n",
    "        return np.nan\n",
    "    return Counter(clusters).most_common(1)[0][0]\n",
    "\n",
    "\n",
    "df['cluster'] = df['categories_list'].apply(assign_primary_cluster)\n",
    "\n",
    "\n",
    "ohe = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "brand_encoded = ohe.fit_transform(df[['brand']])\n",
    "brand_encoded_df = pd.DataFrame(\n",
    "    brand_encoded, columns=[f\"brand_{cat}\" for cat in ohe.categories_[0]])\n",
    "df = pd.concat([df, brand_encoded_df], axis=1)\n",
    "\n",
    "name_embeddings = embedding_model.encode(\n",
    "    df['name'].tolist(), show_progress_bar=True)\n",
    "name_embeddings = np.array(name_embeddings)\n",
    "\n",
    "pca_product = PCA(n_components=50, random_state=42)\n",
    "name_pca = pca_product.fit_transform(name_embeddings)\n",
    "\n",
    "category_cluster_feature = df['cluster'].values.reshape(-1, 1)\n",
    "brand_features = brand_encoded\n",
    "product_features = name_pca\n",
    "\n",
    "combined_features = np.hstack([\n",
    "    category_cluster_feature,\n",
    "    brand_features,\n",
    "    product_features,\n",
    "])\n",
    "\n",
    "combined_scaler = StandardScaler()\n",
    "combined_features_scaled = combined_scaler.fit_transform(combined_features)\n",
    "\n",
    "final_k = 6\n",
    "\n",
    "final_kmeans = KMeans(n_clusters=final_k, random_state=42)\n",
    "final_cluster_labels = final_kmeans.fit_predict(combined_features_scaled)\n",
    "\n",
    "df['final_cluster'] = final_cluster_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_title_mapping = {\n",
    "    0: 'Power and Connectivity Accessories',\n",
    "    1: 'Amazon Devices and Media Equipment',\n",
    "    2: 'Computing and Mobile Devices',\n",
    "    3: 'Home and Office Products',\n",
    "    4: 'Carrying and Storage Accessories',\n",
    "    5: 'Audio Equipment and Accessories'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarization with Bart-base\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-base\",\n",
    "                      tokenizer=\"facebook/bart-base\", device=0)\n",
    "\n",
    "\n",
    "def generate_summary(reviews_text):\n",
    "    input_length = len(reviews_text.split())\n",
    "\n",
    "    max_len = max(5, min(25, input_length - 1))\n",
    "    min_len = max(3, int(max_len * 0.5))\n",
    "\n",
    "    if min_len >= max_len:\n",
    "        min_len = max_len - 1\n",
    "\n",
    "    tokenizer = summarizer.tokenizer\n",
    "    tokenized_text = tokenizer(reviews_text, truncation=True, max_length=1024)\n",
    "    truncated_text = tokenizer.decode(\n",
    "        tokenized_text['input_ids'], skip_special_tokens=True)\n",
    "\n",
    "    if input_length < 5:\n",
    "        return reviews_text\n",
    "\n",
    "    summary = summarizer(\n",
    "        truncated_text,\n",
    "        max_length=max_len,\n",
    "        min_length=min_len,\n",
    "        do_sample=False\n",
    "    )\n",
    "    return summary[0]['summary_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_sentiment(df):\n",
    "    agg_df = df.groupby(['final_cluster', 'name']).agg(\n",
    "        avg_sentiment_score=('sentiment_score', 'mean'),\n",
    "        total_reviews=('sentiment_score', 'count')\n",
    "    ).reset_index()\n",
    "\n",
    "    return agg_df\n",
    "\n",
    "\n",
    "aggregated_metrics = calculate_average_sentiment(df)\n",
    "\n",
    "\n",
    "def get_top_n_best_rated_and_most_reviewed(agg_df, n=3):\n",
    "    top_products = agg_df.sort_values(\n",
    "        ['final_cluster', 'avg_sentiment_score'], ascending=[True, False])\n",
    "    top_n = top_products.groupby('final_cluster').head(n)\n",
    "    most_reviewed = agg_df.sort_values(\n",
    "        ['final_cluster', 'total_reviews'], ascending=[True, False])\n",
    "    top_n_most_reviewed = most_reviewed.groupby('final_cluster').head(n)\n",
    "    return top_n, top_n_most_reviewed\n",
    "\n",
    "\n",
    "top_3_best_rated, top_3_most_reviewed = get_top_n_best_rated_and_most_reviewed(\n",
    "    aggregated_metrics, n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gpt2_comparison_prompt(cluster, top_3, cluster_title_mapping):\n",
    "    category_title = cluster_title_mapping.get(cluster, f\"Cluster {cluster}\")\n",
    "\n",
    "    prompt = []\n",
    "\n",
    "    prompt.append(f\"Category: {category_title}\\n\\n\")\n",
    "    prompt.append(\"## Top 3 Products Comparison\\n\")\n",
    "\n",
    "    for index, row in top_3.iterrows():\n",
    "        prompt.append(f\"- **{row['name']}**: Average rating {\n",
    "                      row['avg_sentiment_score']:.2f} based on {row['total_reviews']} reviews.\\n\")\n",
    "\n",
    "    prompt.append(\"\\n### Key Differences Between the Top 3 Products:\\n\")\n",
    "    prompt.append(\n",
    "        \"Compare the products based on features, price, and customer reviews. Highlight the strengths and weaknesses of each product.\\n\")\n",
    "\n",
    "    return \"\".join(prompt)\n",
    "\n",
    "\n",
    "cluster = 4\n",
    "\n",
    "top_3 = top_3_best_rated[top_3_best_rated['final_cluster'] == cluster]\n",
    "\n",
    "prompt = create_gpt2_comparison_prompt(cluster, top_3, cluster_title_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    input_ids,\n",
    "    max_length=1000,\n",
    "    num_return_sequences=1,\n",
    "    no_repeat_ngram_size=2,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
