{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from collections import Counter\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import datasets\n",
    "import numpy as np\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "datasets.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'nlptown/bert-base-multilingual-uncased-sentiment')\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'nlptown/bert-base-multilingual-uncased-sentiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    '/notebooks/Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products_May19.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows where 'reviews.text' is NaN or contains only whitespace\n",
    "\n",
    "df = df.dropna(subset=['reviews.text']).reset_index(drop=True)\n",
    "df['reviews.text'] = df['reviews.text'].astype(str)\n",
    "df = df[df['reviews.text'].str.strip().astype(bool)].reset_index(drop=True)\n",
    "\n",
    "columns_to_drop_df = ['dateAdded', 'dateUpdated',\n",
    "                      'asins', 'reviews.date', 'manufacturerNumber']\n",
    "df.drop(columns=columns_to_drop_df, inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = df['reviews.text'].tolist()\n",
    "df_reviews = pd.DataFrame(reviews, columns=['review'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "def sentiment_score(review):\n",
    "    with torch.no_grad():\n",
    "        tokens = tokenizer.encode(\n",
    "            review,\n",
    "            return_tensors='pt',\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        ).to(device)\n",
    "\n",
    "        result = model(tokens)\n",
    "\n",
    "        # Get the predicted class (0-4) and convert to 1-5 scale\n",
    "        score = int(torch.argmax(result.logits)) + 1\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "def map_score_to_classification(score):\n",
    "    if score <= 2:\n",
    "        return 'Negative'\n",
    "    elif score == 3:\n",
    "        return 'Neutral'\n",
    "    else:\n",
    "        return 'Positive'\n",
    "\n",
    "\n",
    "sentiment_scores = []\n",
    "sentiment_classifications = []\n",
    "\n",
    "for review in reviews:\n",
    "    score = sentiment_score(review)\n",
    "    classification = map_score_to_classification(score)\n",
    "\n",
    "    sentiment_scores.append(score)\n",
    "    sentiment_classifications.append(classification)\n",
    "\n",
    "df['sentiment_score'] = sentiment_scores\n",
    "df['sentiment_classification'] = sentiment_classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['categories_list'] = df['categories'].apply(\n",
    "    lambda x: [cat.strip() for cat in x.split(',') if cat.strip()])\n",
    "\n",
    "# Consolidate similar categories to eliminate redundancy\n",
    "consolidation_mapping = {\n",
    "    'All Bluetooth & Wireless Speakers': 'Bluetooth & Wireless Speakers',\n",
    "    'All Streaming Media Players': 'Streaming Media Players',\n",
    "    'All Tablets': 'Tablets',\n",
    "    'Amazon Tablets': 'Tablets',\n",
    "    'Amazon Book Reader': 'Book Readers',\n",
    "    'Amazon Book Reader Accessory': 'Book Reader Accessories',\n",
    "    'Amazon Device Accessories': 'Device Accessories',\n",
    "    'Amazon Devices': 'Devices',\n",
    "    'Amazon Devices & Accessories': 'Devices & Accessories',\n",
    "    'Amazon Ereaders': 'Ereaders',\n",
    "    'Amazon Tablet Accessory': 'Tablet Accessories',\n",
    "    'Amazon Tap': 'Amazon Tap',\n",
    "    'Audio Player Accessories': 'Audio Accessories',\n",
    "    'Carrying Case Or Bag': 'Carrying Cases & Bags',\n",
    "    'Cases & Bags': 'Cases & Bags',\n",
    "    'Cases & Covers': 'Cases & Covers',\n",
    "    'Carriers & Crates': 'Carriers & Crates',\n",
    "    'Carriers & Totes': 'Carriers & Totes',\n",
    "    'Cookware': 'Home & Kitchen',\n",
    "    'Kitchen & Dining': 'Home & Kitchen',\n",
    "    'Portable Audio & Headphones': 'Audio',\n",
    "    'Tablets': 'Computers & Tablets',\n",
    "    'iPads': 'Computers & Tablets',\n",
    "    'E-readers': 'Computers & Tablets',\n",
    "}\n",
    "\n",
    "df['categories_list'] = df['categories_list'].apply(\n",
    "    lambda cats: [consolidation_mapping.get(cat, cat) for cat in cats]\n",
    ")\n",
    "\n",
    "# Remove or assign to a meaningful category\n",
    "\n",
    "\n",
    "def clean_category(cat):\n",
    "    if cat in ['14701001', 'AA', 'AAA', 'Abis Electronics', 'Amazon', 'Amazon SMP']:\n",
    "        return None\n",
    "    return cat\n",
    "\n",
    "\n",
    "df['categories_list'] = df['categories_list'].apply(\n",
    "    lambda cats: [clean_category(cat)\n",
    "                  for cat in cats if clean_category(cat) is not None]\n",
    ")\n",
    "\n",
    "# Extract unique categories\n",
    "unique_categories = sorted(\n",
    "    set(cat for cats in df['categories_list'] for cat in cats))\n",
    "\n",
    "\n",
    "# Generate category embeddings using sentence transformers\n",
    "embedding_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "category_embeddings = embedding_model.encode(\n",
    "    unique_categories, show_progress_bar=True)\n",
    "category_embeddings = np.array(category_embeddings)\n",
    "\n",
    "# Cluster categories\n",
    "num_clusters = 6\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(category_embeddings)\n",
    "\n",
    "# Create category to cluster mapping\n",
    "category_cluster_df = pd.DataFrame({\n",
    "    'category': unique_categories,\n",
    "    'category_cluster': cluster_labels\n",
    "})\n",
    "\n",
    "# Map categories to clusters\n",
    "category_to_cluster = dict(\n",
    "    zip(category_cluster_df['category'], category_cluster_df['category_cluster']))\n",
    "\n",
    "# Assign primary cluster to each review\n",
    "\n",
    "\n",
    "def assign_primary_cluster(categories):\n",
    "    clusters = [category_to_cluster.get(\n",
    "        cat) for cat in categories if category_to_cluster.get(cat) is not None]\n",
    "    if not clusters:\n",
    "        return np.nan\n",
    "    return Counter(clusters).most_common(1)[0][0]\n",
    "\n",
    "\n",
    "df['cluster'] = df['categories_list'].apply(assign_primary_cluster)\n",
    "\n",
    "\n",
    "ohe = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "brand_encoded = ohe.fit_transform(df[['brand']])\n",
    "brand_encoded_df = pd.DataFrame(\n",
    "    brand_encoded, columns=[f\"brand_{cat}\" for cat in ohe.categories_[0]])\n",
    "df = pd.concat([df, brand_encoded_df], axis=1)\n",
    "\n",
    "name_embeddings = embedding_model.encode(\n",
    "    df['name'].tolist(), show_progress_bar=True)\n",
    "name_embeddings = np.array(name_embeddings)\n",
    "\n",
    "pca_product = PCA(n_components=50, random_state=42)\n",
    "name_pca = pca_product.fit_transform(name_embeddings)\n",
    "\n",
    "category_cluster_feature = df['cluster'].values.reshape(-1, 1)\n",
    "brand_features = brand_encoded\n",
    "product_features = name_pca\n",
    "\n",
    "combined_features = np.hstack([\n",
    "    category_cluster_feature,\n",
    "    brand_features,\n",
    "    product_features,\n",
    "])\n",
    "\n",
    "combined_scaler = StandardScaler()\n",
    "combined_features_scaled = combined_scaler.fit_transform(combined_features)\n",
    "\n",
    "final_k = 6\n",
    "\n",
    "final_kmeans = KMeans(n_clusters=final_k, random_state=42)\n",
    "final_cluster_labels = final_kmeans.fit_predict(combined_features_scaled)\n",
    "\n",
    "df['final_cluster'] = final_cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_title_mapping = {\n",
    "    0: 'Power and Connectivity Accessories',\n",
    "    1: 'Amazon Devices and Media Equipment',\n",
    "    2: 'Computing and Mobile Devices',\n",
    "    3: 'Home and Office Products',\n",
    "    4: 'Carrying and Storage Accessories',\n",
    "    5: 'Audio Equipment and Accessories'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarization with Bart-base\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-base\",\n",
    "                      tokenizer=\"facebook/bart-base\", device=0)\n",
    "\n",
    "\n",
    "def generate_summary(reviews_text):\n",
    "    input_length = len(reviews_text.split())\n",
    "\n",
    "    max_len = max(5, min(25, input_length - 1))\n",
    "    min_len = max(3, int(max_len * 0.5))\n",
    "\n",
    "    if min_len >= max_len:\n",
    "        min_len = max_len - 1\n",
    "\n",
    "    tokenizer = summarizer.tokenizer\n",
    "    tokenized_text = tokenizer(reviews_text, truncation=True, max_length=1024)\n",
    "    truncated_text = tokenizer.decode(\n",
    "        tokenized_text['input_ids'], skip_special_tokens=True)\n",
    "\n",
    "    if input_length < 5:\n",
    "        return reviews_text\n",
    "\n",
    "    summary = summarizer(\n",
    "        truncated_text,\n",
    "        max_length=max_len,\n",
    "        min_length=min_len,\n",
    "        do_sample=False\n",
    "    )\n",
    "    return summary[0]['summary_text']\n",
    "\n",
    "\n",
    "df['summary'] = df['reviews.text'].apply(\n",
    "    lambda x: generate_summary(x) if pd.notnull(x) else \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate pros and cons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m nlp \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43men_core_web_sm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_pros_cons_spacy\u001b[39m(reviews, sentiments):\n\u001b[1;32m      5\u001b[0m     pros \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/spacy/__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[1;32m     28\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[1;32m     35\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[1;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/spacy/util.py:472\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE941\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, full\u001b[38;5;241m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[0;32m--> 472\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE050\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "def extract_pros_cons_spacy(reviews, sentiments):\n",
    "    pros = []\n",
    "    cons = []\n",
    "\n",
    "    positive_reviews = [reviews[i] for i in range(\n",
    "        len(reviews)) if sentiments[i] == 'Positive']\n",
    "    negative_reviews = [reviews[i] for i in range(\n",
    "        len(reviews)) if sentiments[i] == 'Negative']\n",
    "\n",
    "    for review in positive_reviews:\n",
    "        doc = nlp(review)\n",
    "        for token in doc:\n",
    "            if token.pos_ == 'ADJ' and token.dep_ in ['amod', 'acomp']:\n",
    "                pros.append(token.text.lower())\n",
    "\n",
    "    for review in negative_reviews:\n",
    "        doc = nlp(review)\n",
    "        for chunk in doc.noun_chunks:\n",
    "            if any(token.dep_ == 'neg' for token in chunk.root.lefts) or any(token.text.lower() in ['problem', 'issue', 'bad', 'poor'] for token in chunk):\n",
    "                cons.append(chunk.text.lower())\n",
    "\n",
    "    top_pros = [word for word, count in Counter(pros).most_common(10)]\n",
    "    top_cons = [phrase for phrase, count in Counter(cons).most_common(10)]\n",
    "\n",
    "    return top_pros, top_cons\n",
    "\n",
    "\n",
    "pros_cons_list = []\n",
    "for cluster_id in df['final_cluster'].unique():\n",
    "    top_product = df[df['final_cluster']\n",
    "                     == cluster_id].iloc[0]\n",
    "    product_name = top_product['name']\n",
    "\n",
    "    reviews = df[df['final_cluster']\n",
    "                 == cluster_id]['reviews.text'].tolist()\n",
    "    sentiments = df[df['final_cluster']\n",
    "                    == cluster_id]['sentiment_classification'].tolist()\n",
    "\n",
    "    # Extract pros and cons using spaCy\n",
    "    pros, cons = extract_pros_cons_spacy(reviews, sentiments)\n",
    "\n",
    "    # Store the results\n",
    "    pros_cons_list.append({\n",
    "        'cluster': cluster_id,\n",
    "        'product_name': product_name,\n",
    "        'pros': pros,\n",
    "        'cons': cons\n",
    "    })\n",
    "\n",
    "# Display or save the pros and cons for each top product\n",
    "for entry in pros_cons_list:\n",
    "    print(f\"Cluster {entry['cluster']} - {entry['product_name']}\")\n",
    "    print(\"Pros: \", ', '.join(entry['pros']))\n",
    "    print(\"Cons: \", ', '.join(entry['cons']))\n",
    "    print()\n",
    "\n",
    "\n",
    "# pros_cons_df = pd.DataFrame(pros_cons_list)\n",
    "# pros_cons_df.to_csv('product_pros_and_cons_spacy.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_38/2518922005.py:1: DtypeWarning: Columns (14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.85` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:418: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\n",
    "    '/notebooks/df_reviews_with_summaries_and_clusters_sentiment.csv')\n",
    "\n",
    "# Load the CSV containing pros and cons\n",
    "pros_cons_df = pd.read_csv('product_pros_and_cons_spacy.csv')\n",
    "pros_cons_list = pros_cons_df.to_dict('records')\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# Sort the df by 'final_cluster' (ascending) and 'sentiment_score' (descending)\n",
    "df = df.sort_values(\n",
    "    ['final_cluster', 'sentiment_score'], ascending=[True, False])\n",
    "\n",
    "\n",
    "def generate_product_summary(product_name, product_category):\n",
    "    prompt = (\n",
    "        f\"The {product_name} from the \"\n",
    "        f\"{product_category} category is built to offer excellent performance. \"\n",
    "        \"It stands out for its reliability, durability, and value for money. Here's a short overview of this product, outlining its key benefits.\"\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True,\n",
    "                       truncation=True)\n",
    "    outputs = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_length=200,\n",
    "        temperature=0.6,\n",
    "        top_p=0.85,\n",
    "        no_repeat_ngram_size=3,\n",
    "        early_stopping=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "def generate_product_features(product_name):\n",
    "    prompt = (\n",
    "        f\"The key features of the \"\n",
    "        f\"{product_name} include its high performance, durable design, and superior functionality. \"\n",
    "        \"This product is ideal for consumers who need reliable technology. It offers great value when compared to competitors in the same category.\"\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(\n",
    "        inputs,\n",
    "        max_length=200,\n",
    "        temperature=0.6,\n",
    "        top_p=0.85,\n",
    "        no_repeat_ngram_size=2,\n",
    "        early_stopping=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "\n",
    "    )\n",
    "\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# Generate the blog post\n",
    "def generate_blog_post(product_name, product_category, pros, cons):\n",
    "    summary = generate_product_summary(product_name, product_category)\n",
    "    features = generate_product_features(product_name)\n",
    "\n",
    "    blog_post = (\n",
    "        f\"## Product Overview\\n\\n{summary}\\n\\n\"\n",
    "        f\"## Key Features\\n\\n{features}\\n\\n\"\n",
    "        f\"## Pros\\n\\n- \" + '\\n- '.join(pros) + \"\\n\\n\"\n",
    "        f\"## Cons\\n\\n- \" + '\\n- '.join(cons) + \"\\n\\n\"\n",
    "        f\"## Conclusion\\n\\n\"\n",
    "        f\"In conclusion, the {product_name} offers excellent \"\n",
    "        f\"{pros[0]} and {pros[1]}, \"\n",
    "        f\"but keep in mind that it has some drawbacks such as {cons[0]}.\"\n",
    "    )\n",
    "\n",
    "    return blog_post\n",
    "\n",
    "\n",
    "# Generate blog posts for the top 1 product in each cluster\n",
    "blog_posts = []\n",
    "for cluster_id in df['final_cluster'].unique():\n",
    "    # Select the top product from each cluster (sorted by sentiment score)\n",
    "    top_product = df[df['final_cluster'] == cluster_id].iloc[0]\n",
    "\n",
    "    product_name = top_product['name']\n",
    "    product_category = cluster_title_mapping.get(\n",
    "        cluster_id, \"Miscellaneous Products\")\n",
    "\n",
    "    # Get the pros and cons for this product\n",
    "    entry = next(\n",
    "        (item for item in pros_cons_list if item['product_name'] == product_name), None)\n",
    "\n",
    "    if entry:\n",
    "        pros = entry['pros'].split(',')\n",
    "        cons = entry['cons'].split(',')\n",
    "\n",
    "        # Generate the blog post\n",
    "        blog_post = generate_blog_post(\n",
    "            product_name, product_category, pros, cons)\n",
    "\n",
    "        # Append to blog_posts list\n",
    "        blog_posts.append({\n",
    "            'cluster': cluster_id,\n",
    "            'product_name': product_name,\n",
    "            'blog_post': blog_post\n",
    "        })\n",
    "\n",
    "# Output the blog posts\n",
    "for post in blog_posts:\n",
    "    print(f\"Cluster {post['cluster']} - {post['product_name']}\")\n",
    "    print(post['blog_post'])\n",
    "    print()\n",
    "\n",
    "\n",
    "blog_posts_df = pd.DataFrame(blog_posts)\n",
    "blog_posts_df.to_csv('generated_blog_posts1.csv', index=False)\n",
    "\n",
    "\n",
    "# blog_posts_df = pd.DataFrame(blog_posts)\n",
    "# blog_posts_df.to_csv('../data/generated_blog_posts.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
